"""
Script Ä‘á»ƒ táº¡o embeddings sá»­ dá»¥ng SentenceTransformers vá»›i faq_dataset.json
"""
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import os
from tqdm import tqdm

def load_faq_data(file_path):
    """Load FAQ data from JSON file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… ÄÃ£ load {len(data)} cÃ¢u há»i tá»« {file_path}")
        return data
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {file_path}: {e}")
        return None

def create_embeddings_with_sentence_transformers(data, model_name='all-MiniLM-L6-v2'):
    """
    Táº¡o embeddings sá»­ dá»¥ng SentenceTransformers
    Model options:
    - 'all-MiniLM-L6-v2': Nháº¹, nhanh, hiá»‡u suáº¥t tá»‘t (384 dimensions)
    - 'all-mpnet-base-v2': Hiá»‡u suáº¥t cao hÆ¡n (768 dimensions)
    - 'paraphrase-multilingual-MiniLM-L12-v2': Há»— trá»£ tiáº¿ng Viá»‡t tá»‘t
    """
    print(f"ğŸ¤– Äang load model SentenceTransformers: {model_name}")
    
    # Load model
    model = SentenceTransformer(model_name)
    
    # Chuáº©n bá»‹ dá»¯ liá»‡u
    questions = []
    answers = []
    categories = []
    tags = []
    
    for item in data:
        questions.append(item['question'])
        answers.append(item['answer'])
        categories.append(item.get('category', 'general'))
        tags.append(item.get('tags', []))
    
    print(f"ğŸ“ Äang táº¡o embeddings cho {len(questions)} cÃ¢u há»i...")
    
    # Táº¡o embeddings cho cÃ¢u há»i
    question_embeddings = model.encode(
        questions, 
        show_progress_bar=True,
        batch_size=32,
        convert_to_numpy=True
    )
    
    # Táº¡o DataFrame
    df = pd.DataFrame({
        'question': questions,
        'answer': answers,
        'category': categories,
        'tags': tags,
        'embedding': question_embeddings.tolist()
    })
    
    print(f"âœ… ÄÃ£ táº¡o embeddings vá»›i shape: {question_embeddings.shape}")
    return df, model

def save_embeddings(df, output_path):
    """LÆ°u embeddings vÃ o file parquet"""
    try:
        df.to_parquet(output_path, index=False)
        print(f"âœ… ÄÃ£ lÆ°u embeddings vÃ o {output_path}")
        print(f"ğŸ“Š Sá»‘ dÃ²ng: {len(df)}")
        print(f"ğŸ”¢ CÃ¡c cá»™t: {list(df.columns)}")
        return True
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file: {e}")
        return False

def main():
    # ÄÆ°á»ng dáº«n files
    faq_file = './faq_dataset.json'
    output_file = './english_qa_embeddings.parquet'
    
    # Kiá»ƒm tra file tá»“n táº¡i
    if not os.path.exists(faq_file):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {faq_file}")
        return
    
    # Load dá»¯ liá»‡u
    faq_data = load_faq_data(faq_file)
    if not faq_data:
        return
    
    # Táº¡o embeddings
    try:
        df, model = create_embeddings_with_sentence_transformers(
            faq_data, 
            model_name='paraphrase-multilingual-MiniLM-L12-v2'  # Tá»‘t cho tiáº¿ng Viá»‡t
        )
        
        # LÆ°u káº¿t quáº£
        if save_embeddings(df, output_file):
            print(f"\nğŸ‰ HoÃ n thÃ nh! File Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_file}")
            print(f"ğŸ“ Embedding dimension: {len(df['embedding'].iloc[0])}")
            print(f"ğŸ·ï¸ Categories: {df['category'].unique()}")
            
            # Hiá»ƒn thá»‹ má»™t vÃ i vÃ­ dá»¥
            print("\nğŸ“‹ Má»™t vÃ i vÃ­ dá»¥:")
            for i in range(min(3, len(df))):
                print(f"  {i+1}. Q: {df['question'].iloc[i][:50]}...")
                print(f"     A: {df['answer'].iloc[i][:50]}...")
                print(f"     Category: {df['category'].iloc[i]}")
                print()
        
    except Exception as e:
        print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh táº¡o embeddings: {e}")

if __name__ == "__main__":
    main()
